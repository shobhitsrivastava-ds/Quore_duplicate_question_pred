{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Identifying the Duplicate Questions\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## Importing the suitable libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nstp= stopwords.words()\nfrom nltk.stem import WordNetLemmatizer\nfrom matplotlib import style\nfrom collections import Counter\nstyle.use(\"fivethirtyeight\")\n%matplotlib inline\nlemm= WordNetLemmatizer()\nfrom keras import layers\nfrom keras import Model\nfrom keras.utils import np_utils\nfrom  keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import Input\nfrom keras.utils import to_categorical\nimport string\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Importing the libraries\ndata= pd.read_csv(\"../input/train.csv\")\ndata_t= pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Printing the data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_unigrams(text):\n    return([word.lower() for word in text.split() if w not in stp])\ndef count_common(text):\n    return(len(set(text[\"question1_m\"]).intersection(set(text[\"question2_m\"]))))\ndef get_common_unigram_ratio(row):\n    return float(row[\"common_w\"]) / max(len( set(row[\"question1_m\"]).union(set(row[\"question2_m\"]))),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"common_w\"]= data.apply(lambda x: count_common(x),axis=1)\ndata[\"common_ratio\"]= data.apply(lambda x: get_common_unigram_ratio(x),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data[\"common_ratio\"], kde=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc= data[\"common_w\"].value_counts()\nsns.barplot(wc.index, wc.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Function to filter out the stopwords and irrelvent punctuations\ndef filter_q(sent):\n    table= str.maketrans(\"\",\"\",string.punctuation)\n    strr= [w.translate(table) for w in str(sent).split()]\n    strr= [w.lower() for w in strr]\n    strr= [lemm.lemmatize(w) for w in strr if w not in stp]\n    strr= [w for w in strr if w.isalpha()]\n    strr= [w for w in strr if len(w)>2]\n    t_sent= \" \".join(strr)\n    return(t_sent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"question1_m\"]= data[\"question1\"].apply(filter_q)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"question2_m\"]= data[\"question2\"].apply(filter_q)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token1= Tokenizer()\ntoken2= Tokenizer()\ntoken1.fit_on_texts(data[\"question1_m\"])\ntoken2.fit_on_texts(data[\"question2_m\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_w1= len(token1.word_index)+1\ntotal_w2= len(token2.word_index)+1\nmax_q1= max([len(i) for i in quest_1])\nmax_q2= max([len(j) for j in quest_2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quest_1= token1.texts_to_sequences(data[\"question1_m\"])\nquest_2= token2.texts_to_sequences(data[\"question2_m\"])\nquest_1= pad_sequences(quest_1, maxlen= max_q1)\nquest_2= pad_sequences(quest_2, maxlen= max_q2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Base model :"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Question1\ninput_1= Input(shape=(None,), dtype=\"int32\",name=\"ques1\")\nembedd_text= layers.Embedding(total_w1,64)(input_1)\nlstm_l1= layers.LSTM(62,recurrent_dropout=0.2)(embedd_text)\n## Question2\ninput_2= Input(shape=(None,), dtype=\"int32\",name=\"ques2\")\nembedd_text= layers.Embedding(total_w2,96)(input_2)\nlstm_l2= layers.LSTM(62, recurrent_dropout=0.2)(embedd_text)\nconcat= layers.concatenate([lstm_l1,lstm_l2],axis=1)\nanswer= layers.Dense(2,activation=\"softmax\")(concat)\nmodel = Model([input_1, input_2], answer)\nmodel.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output =np_utils.to_categorical(data[\"is_duplicate\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit([quest_1,quest_2],output,epochs=2,batch_size=128,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data[\"is_duplicate\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(data[\"qid1\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ser= pd.Series(data[\"question1\"].tolist() + data[\"question2\"].tolist()).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_trqser = ser.apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_trqser.plot(\"hist\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ser1= data[\"question1_m\"].astype(\"str\")\nser2= data[\"question2_m\"].astype(\"str\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"serw_1=  \" \".join(ser1)\nserw_2= \" \".join(ser2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nw_c= WordCloud(stopwords= STOPWORDS, background_color=\"white\", height= 2000, width =4000).generate(serw_1)\nplt.imshow(w_c),\nplt.figure(figsize=(16,8))\nplt.axis(\"off\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc_2= WordCloud(stopwords= STOPWORDS, background_color=\"white\", height= 2000, width=4000).generate(serw_2)\nplt.imshow(wc_2)\nplt.figure(figsize=(16,8))\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data[\"is_duplicate\"],data[\"common_ratio\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data[\"is_duplicate\"],data[\"common_w\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Thus from the above two plot we can estimate that common ratio is the best estimator to differentiate between duplicate and non duplicate question"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nq_dict= defaultdict(set)\nques= pd.concat([data[[\"question1\",\"question2\"]],data_t[[\"question1\",\"question2\"]]], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ques","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nq_dict= defaultdict(set)\nfor i in range(ques.shape[0]):\n    q_dict[ques.question1[i].add(ques.question2[i])]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_dict[\"a\"].add(\"b\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_l(a):\n    return(len(str(a).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ques[\"q1_l\"]= ques[\"question1\"].apply(find_l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"ques1_len\"] = data[\"question1\"].apply(find_l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"ques2_len\"]= data[\"question2\"].apply(find_l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max(data[\"ques2_len\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pvt_df= data.pivot_table(index= \"ques1_len\", columns=\"ques2_len\", values= \"is_duplicate\")\nsns.heatmap(pvt_df)\nplt.title(\"Mean is_duplicate value distribution acrosss q1 and q2 frequency\")\nplt.figure(figsize=(12,12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nval = ques[\"q1_l\"].value_counts()\nsns.barplot(val.index,val.values, alpha=0.5)\nplt.xticks(rotation=\"vertical\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,18))\ndata_glf= data.groupby(\"ques1_len\")[\"is_duplicate\"].agg(\"mean\")\nsns.barplot(data[\"ques1_len\"].value_counts().values, data_glf.values)\nplt.xticks(rotation=\"vertical\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Question1\ninput_1= Input(shape=(None,), dtype=\"int32\",name=\"ques1\")\nembedd_text= layers.Embedding(total_w1,64)(input_1)\nlstm_l1= layers.LSTM(62,recurrent_dropout=0.2)(embedd_text)\n## Question2\ninput_2= Input(shape=(None,), dtype=\"int32\",name=\"ques2\")\nembedd_text= layers.Embedding(total_w2,96)(input_2)\nlstm_l2= layers.LSTM(62, recurrent_dropout=0.2)(embedd_text)\n\ninput_3= Input(shape=(None,),dtype=\"int32\",name=\"common_w\")\nden_2= layers.Dense(2,activation=\"softmax\")(input_3)\n\nconcat= layers.concatenate([lstm_l1,lstm_l2,den_2],axis=1)\nanswer= layers.Dense(2,activation=\"softmax\")(concat)\nmodel = Model([input_1, input_2, input_3], answer)\nmodel.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit([quest_1,quest_2,data[\"common_ratio\"]],output,epochs=2,batch_size=128,verbose=1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}